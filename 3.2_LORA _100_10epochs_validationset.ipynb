{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVk5XL5cErVa",
        "outputId": "4d85729f-11b7-436d-aa62-11302d33df95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 10 11:17:51 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0              63W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the datasets library\n",
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsaZFg5NCp3K",
        "outputId": "c500c53d-0a6b-47ba-8ff1-190abead6814"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bB5VCz_cB2sH"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- DEFINE LORA Layer**\n",
        "\n",
        "Define the custom Low-Rank Adaptation (LoRA) layer."
      ],
      "metadata": {
        "id": "0RopgfU7CH9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LoRA Layer\n",
        "class LoRALayer(nn.Module):\n",
        "    def __init__(self, input_dim, rank=4):\n",
        "        super(LoRALayer, self).__init__()\n",
        "        self.rank = rank\n",
        "        self.A = nn.Parameter(torch.randn(input_dim, rank))\n",
        "        self.B = nn.Parameter(torch.randn(rank, input_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + torch.matmul(torch.matmul(x, self.A), self.B)\n",
        ""
      ],
      "metadata": {
        "id": "e9PSF8yZB4nB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Modify GPT-2 to Include LoRA Layers\n",
        "Create a new class to modify the GPT-2 model to include the LoRA layers."
      ],
      "metadata": {
        "id": "Lw-apTj8WnL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify GPT-2 to include LoRA layers\n",
        "class GPT2WithLoRA(nn.Module):\n",
        "    def __init__(self, model_name='gpt2', rank=4):\n",
        "        super(GPT2WithLoRA, self).__init__()\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.rank = rank\n",
        "        self.add_lora_layers()\n",
        "\n",
        "    def add_lora_layers(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                input_dim = module.in_features\n",
        "                lora_layer = LoRALayer(input_dim, self.rank)\n",
        "                module.add_module('lora', lora_layer)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        ""
      ],
      "metadata": {
        "id": "nQvv4aOOB426"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Load Pre-trained Model and Tokenizer\n",
        "Load the pre-trained GPT-2 model and tokenizer, and set the padding token."
      ],
      "metadata": {
        "id": "vv7im33rEb7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqtPU1DoOJ_-",
        "outputId": "3ba41289-11e0-4ce7-a871-1df65d42a45b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set EOS token as pad token\n",
        "model = GPT2WithLoRA('gpt2').to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIA-SZUuB5Ei",
        "outputId": "1a5e40ee-07c0-4f1e-a8d0-d68e00843afd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Define Optimizer and Loss Function\n",
        "Set up the optimizer and loss function."
      ],
      "metadata": {
        "id": "iuv3jhkwEfyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QSVW2YXB5Lq",
        "outputId": "51b398e5-2bb4-49d1-d176-7f046a33531d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Load and Prepare Dataset\n",
        "Load the WikiText-103 dataset, sample a subset, and prepare it for training."
      ],
      "metadata": {
        "id": "GI5LFeCYE11Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load WikiText-103 dataset\n",
        "wikitext = load_dataset('wikitext', 'wikitext-103-v1')\n",
        "\n",
        "# Convert the dataset to a list of dictionaries\n",
        "train_samples = [sample for sample in wikitext['train']]\n",
        "\n",
        "# Use a small sample of 100 rows for quick testing\n",
        "sample_size = 100\n",
        "train_sampled = random.sample(train_samples, sample_size)\n",
        "train_df = pd.DataFrame(train_sampled)\n",
        "\n"
      ],
      "metadata": {
        "id": "E-q5SUB3B5SJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1)"
      ],
      "metadata": {
        "id": "uXXaOC5ePbvY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Prepare Dataset for the Model\n",
        "Prepare the dataset using a custom TextDataset class and a data loader with padding handled by a data collator."
      ],
      "metadata": {
        "id": "Qr2k5NwBE7ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset for the model\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts.iloc[idx]['text']\n",
        "        inputs = self.tokenizer(text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "        input_ids = inputs['input_ids'].squeeze().long()  # Ensure LongTensor\n",
        "        attention_mask = inputs['attention_mask'].squeeze().long()  # Ensure LongTensor\n",
        "        return input_ids, attention_mask, input_ids\n",
        "\n",
        "# Create DataLoader for training and validation datasets\n",
        "train_dataset = TextDataset(train_df, tokenizer)\n",
        "val_dataset = TextDataset(val_df, tokenizer)"
      ],
      "metadata": {
        "id": "uloZMlpmB5Xr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom collator function to handle padding and formatting\n",
        "def custom_collate_fn(batch):\n",
        "    input_ids = [item[0] for item in batch]\n",
        "    attention_masks = [item[1] for item in batch]\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id).long()\n",
        "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0).long()\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id).long()\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_masks,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "J6ACuy8II948"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Training Loop\n",
        "Train the model with LoRA for a specified number of epochs and print the average loss."
      ],
      "metadata": {
        "id": "9wuetXsyFCcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for the model with LoRA\n",
        "model.train()\n",
        "num_epochs = 10\n",
        "patience = 2\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Skip empty batches\n",
        "        if input_ids.size(0) == 0 or input_ids.size(1) == 0:\n",
        "            continue\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Skip empty batches\n",
        "            if input_ids.size(0) == 0 or input_ids.size(1) == 0:\n",
        "                continue\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # Save the model if validation loss decreases\n",
        "        torch.save(model.state_dict(), 'gpt2_with_lora_best_state_dict.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    model.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5k4RMutFDRr",
        "outputId": "f7a74ae7-0e3e-4926-9553-bb30fbeeea13"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.2259\n",
            "Epoch 1, Validation Loss: 0.6475\n",
            "Epoch 2, Loss: 0.9119\n",
            "Epoch 2, Validation Loss: 0.6348\n",
            "Epoch 3, Loss: 0.7966\n",
            "Epoch 3, Validation Loss: 0.6065\n",
            "Epoch 4, Loss: 0.7550\n",
            "Epoch 4, Validation Loss: 0.5976\n",
            "Epoch 5, Loss: 0.7093\n",
            "Epoch 5, Validation Loss: 0.5919\n",
            "Epoch 6, Loss: 0.6493\n",
            "Epoch 6, Validation Loss: 0.5867\n",
            "Epoch 7, Loss: 0.6886\n",
            "Epoch 7, Validation Loss: 0.5822\n",
            "Epoch 8, Loss: 0.6381\n",
            "Epoch 8, Validation Loss: 0.5784\n",
            "Epoch 9, Loss: 0.6117\n",
            "Epoch 9, Validation Loss: 0.5753\n",
            "Epoch 10, Loss: 0.6128\n",
            "Epoch 10, Validation Loss: 0.5730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Save the Model\n",
        "Save the fine-tuned model and tokenizer."
      ],
      "metadata": {
        "id": "GGbkYjoUFGOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model's state dictionary\n",
        "\n",
        "torch.save(model.state_dict(), 'gpt2_with_lora_state_dict.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jv1ufZ9wFIAj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer\n",
        "\n",
        "tokenizer.save_pretrained('gpt2_with_lora_tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGFgPY3pLSPN",
        "outputId": "668bc25b-a839-48ae-a9a2-5f1984f59db5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt2_with_lora_tokenizer/tokenizer_config.json',\n",
              " 'gpt2_with_lora_tokenizer/special_tokens_map.json',\n",
              " 'gpt2_with_lora_tokenizer/vocab.json',\n",
              " 'gpt2_with_lora_tokenizer/merges.txt',\n",
              " 'gpt2_with_lora_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "-Imports: Import necessary libraries for model handling, dataset loading, and training.\n",
        "\n",
        "-LoRA Layer: Define a custom layer that adds low-rank adaptation to the model.\n",
        "\n",
        "-Model Modification: Modify GPT-2 to include the LoRA layers.\n",
        "\n",
        "-Load Model: Load the pre-trained GPT-2 model and set the padding token.\n",
        "\n",
        "-Optimizer and Loss: Set up the optimizer and loss function for training.\n",
        "\n",
        "-Dataset Loading: Load and sample the WikiText-103 dataset for quick testing.\n",
        "\n",
        "-Dataset Preparation: Prepare the dataset and data loader with padding handled by a data collator.\n",
        "\n",
        "-Training Loop: Train the model and print the loss for each epoch.\n",
        "\n",
        "-Save Model: Save the fine-tuned model and tokenizer for later use.\n"
      ],
      "metadata": {
        "id": "baNKzKGuFJ_H"
      }
    }
  ]
}